akka {
  kafka.producer {
    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 100

    # How long to wait for `KafkaProducer.close`
    close-timeout = 60s

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients {
    }
  }

  kafka.consumer {
    # Tuning property of scheduled polls.
    poll-interval = 500ms

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that blocking of the thread that
    # is executing the stage will be blocked.
    poll-timeout = 50ms

    # The stage will be await outstanding offset commit requests before
    # shutting down, but if that takes longer than this timeout it will
    # stop forcefully.
    stop-timeout = 30s

    # How long to wait for `KafkaConsumer.close`
    close-timeout = 20s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `TimeoutException`.
    commit-timeout = 15s

    # If the KafkaConsumer can't connect to the broker the poll will be
    # aborted after this timeout. The KafkaConsumerActor will throw
    # org.apache.kafka.common.errors.WakeupException which will be ignored
    # until max-wakeups limit gets exceeded.
    wakeup-timeout = 3s

    # After exceeding maxinum wakeups the consumer will stop and the stage will fail.
    max-wakeups = 10

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

  }
}

hydra {
  kafka {
    health_check.interval = 30s
    heath_check.interval = ${?HYDRA_KAFKA_HEALTH_CHECK_INTERVAL}
    consumer {
      key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      group.id = "hydra"
      zookeeper.connect = "localhost:2181"
      zookeeper.connect = ${?HYDRA_ZOOKEEPER_QUORUM}
      auto.offset.reset = latest
    }

    producer {
      bootstrap.servers = "localhost:9092"
      bootstrap.servers = ${?HYDRA_KAFKA_PRODUCER_BOOTSTRAP_SERVERS}
      key.serializer = org.apache.kafka.common.serialization.StringSerializer
      max.in.flight.requests.per.connection = 1
    }

    clients {
      string.producer {
        value.serializer = org.apache.kafka.common.serialization.StringSerializer
      }
      string.consumer {
        value.deserializer = org.apache.kafka.common.serialization.StringSerializer
      }
      avro.producer {
        value.serializer = "io.confluent.kafka.serializers.KafkaAvroSerializer"
        schema.registry.url = ${hydra.schema.registry.url}
      }
      avro.consumer {
        value.deserializer = "io.confluent.kafka.serializers.KafkaAvroDeserializer"
        schema.registry.url = ${hydra.schema.registry.url}
      }
      json.producer {
        value.serializer = org.apache.kafka.common.serialization.StringSerializer
      }
      json.consumer {
        value.deserializer = org.apache.kafka.common.serialization.StringSerializer
      }
    }
  }

}

