akka {
  kafka.producer {
    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 100

    # How long to wait for `KafkaProducer.close`
    close-timeout = 60s

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients {
    }
  }

  kafka.consumer {
    # Tuning property of scheduled polls.
    poll-interval = 500ms

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that blocking of the thread that
    # is executing the stage will be blocked.
    poll-timeout = 50ms

    # The stage will be await outstanding offset commit requests before
    # shutting down, but if that takes longer than this timeout it will
    # stop forcefully.
    stop-timeout = 30s

    # How long to wait for `KafkaConsumer.close`
    close-timeout = 20s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `TimeoutException`.
    commit-timeout = 15s

    # If the KafkaConsumer can't connect to the broker the poll will be
    # aborted after this timeout. The KafkaConsumerActor will throw
    # org.apache.kafka.common.errors.WakeupException which will be ignored
    # until max-wakeups limit gets exceeded.
    wakeup-timeout = 3s

    # After exceeding maxinum wakeups the consumer will stop and the stage will fail.
    max-wakeups = 10

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

  }
}

hydra {
  ingest.classpath-scan = ["hydra.kafka.ingestors"]

  kafka {

    consumer {
      //the default serializers for a consumer
      key.serializer = org.apache.kafka.common.serialization.StringSerializer
      key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      value.serializer = org.apache.kafka.common.serialization.StringSerializer
      value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
      group.id = "hydra"
    }

    # These properties are merged with with the kafka cluster properties.
    formats {
      avro {
        key.serializer = org.apache.kafka.common.serialization.StringSerializer
        key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
        value.serializer = io.confluent.kafka.serializers.KafkaAvroSerializer
        value.deserializer = io.confluent.kafka.serializers.KafkaAvroDeserializer
        client.id = "hydra.avro"
        key.decoder = kafka.serializer.StringDecoder
        value.decoder = io.confluent.kafka.serializers.KafkaAvroDecoder
      }
      json {
        key.serializer = org.apache.kafka.common.serialization.StringSerializer
        key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
        value.serializer = org.apache.kafka.common.serialization.StringSerializer
        value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
        key.decoder = kafka.serializer.StringDecoder
        value.decoder = kafka.serializer.StringDecoder
        client.id = "hydra.json"
      }
      string {
        key.serializer = org.apache.kafka.common.serialization.StringSerializer
        key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
        value.serializer = org.apache.kafka.common.serialization.StringSerializer
        value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
        client.id = "hydra.string"
        key.decoder = kafka.serializer.StringDecoder
        value.decoder = kafka.serializer.StringDecoder
      }
    }
  }

}

